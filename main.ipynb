{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Pick-and-Place Environment in Robosuite\n",
    "\n",
    "## Abstract\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Blah Blah about robosuite and Stable Baselines\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "1. Import needed dependencies: numpy, os, robosuite, stable_baselines 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import robosuite as suite\n",
    "import torch\n",
    "\n",
    "from stable_baselines3.common.save_util import save_to_zip_file, load_from_zip_file\n",
    "from robosuite.controllers import load_controller_config\n",
    "from stable_baselines3 import PPO, DDPG, SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.callbacks import CallbackList, EvalCallback, CheckpointCallback\n",
    "from robosuite.wrappers import GymWrapper\n",
    "\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS backend is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS backend is not available, using CPU.\")\n",
    "    \n",
    "config = {}\n",
    "print('Load configuration file config.yaml')\n",
    "with open(\"config.yaml\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Running environment with training =\", str(config[\"training\"]), \" and simulation =\", str(config[\"simulation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating Standardized Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make robosuite environment into a gym environment as stable baselines only supports gym environments\n",
    "def make_env(env_id, options, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param options: (dict) additional arguments to pass to the specific environment class initializer\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = GymWrapper(suite.make(env_id, **options))\n",
    "        env.render_mode = 'mujoco'\n",
    "        env = Monitor(env)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "#Setup appropriate controller\n",
    "controller_config = load_controller_config(default_controller=config[\"robot_controller\"])\n",
    "\n",
    "'''\n",
    "# Modify the controller configuration to include variable impedance\n",
    "controller_config[\"impedance_mode\"] = \"variable\"\n",
    "controller_config[\"kp_limits\"] = [0, 300]\n",
    "controller_config[\"damping_ratio_limits\"] = [0, 10]\n",
    "'''\n",
    "\n",
    "# Define environment parameters for specific environment \"PickPlace\"\n",
    "env_options = {\"robots\":config[\"robot_name\"],\n",
    "    \"controller_configs\":controller_config,\n",
    "    \"has_renderer\":True,\n",
    "    \"has_offscreen_renderer\":True,\n",
    "    \"use_camera_obs\":False,\n",
    "    \"reward_shaping\":True,\n",
    "    \"horizon\": config[\"horizon\"],\n",
    "    \"control_freq\": config[\"control_freq\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build model and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint, so model can start training where it left off\n",
    "log_dir = \"./logs/\"\n",
    "\n",
    "#Set up TensorBoard logger\n",
    "#tensor_logger = configure('./logs/', [\"tensorboard\"])\n",
    "tensor_logger = log_dir + \"tensorboard\"\n",
    "print(\"TensorBoard logging to\", tensor_logger)\n",
    "\n",
    "'''\n",
    "def find_latest_checkpoint(log_dir, file_type=\".zip\", object_type=None):\n",
    "    files = [f for f in os.listdir(log_dir) if f.startswith(f\"{config['algorithm']}_model_\") and (object_type is None or object_type in f) and f.endswith(file_type)]\n",
    "    files.sort()\n",
    "    if len(files) == 0:\n",
    "        return None\n",
    "    latest_checkpoint = files[-1]\n",
    "    return os.path.join(log_dir, latest_checkpoint)\n",
    "'''\n",
    "\n",
    "#Initialize environment\n",
    "if config[\"multiprocessing\"]:\n",
    "    # Create as many environments to be trained on as there are CPUs/Cores\n",
    "    env = SubprocVecEnv([make_env(\"PickPlace\", env_options, i, config[\"seed\"]) for i in range(config[\"num_envs\"])], start_method= 'spawn')\n",
    "    if config[\"training\"]:\n",
    "        eval_env = SubprocVecEnv([make_env(\"PickPlace\", env_options, i, config[\"seed\"]) for i in range(config[\"num_envs\"])], start_method= 'spawn')\n",
    "        eval_env = VecNormalize(eval_env)\n",
    "else:\n",
    "    env = DummyVecEnv([make_env(\"PickPlace\", env_options, i, config[\"seed\"]) for i in range(config[\"num_envs\"])])\n",
    "    if config[\"training\"]:\n",
    "        eval_env = DummyVecEnv([make_env(\"PickPlace\", env_options, i, config[\"seed\"]) for i in range(config[\"num_envs\"])])\n",
    "        eval_env = VecNormalize(eval_env)\n",
    "if config[\"normalize\"]:\n",
    "    # Wraps the environment to normalize observations and rewards and supports moving average\n",
    "    env = VecNormalize(env)\n",
    "\n",
    "#Initialize model:\n",
    "\n",
    "#if config[\"training\"] and find_latest_checkpoint(log_dir) is None:\n",
    "if config[\"training\"] and not os.path.isfile(config[\"model_file_name\"] + \".zip\"):\n",
    "    print(\"No model found, creating a new one\")\n",
    "\n",
    "    # Create model\n",
    "    if config[\"algorithm\"] == \"PPO\":\n",
    "        model = PPO(config[\"policy\"], env, verbose=1, batch_size=config[\"batch_size\"], tensorboard_log=tensor_logger, device=device)\n",
    "    elif config[\"algorithm\"] == \"DDPG\":\n",
    "        n_actions = env.action_space.shape[-1]\n",
    "        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "        model = DDPG(config[\"policy\"], env, action_noise=action_noise, verbose=1, batch_size=config[\"batch_size\"], tensorboard_log=tensor_logger, device=device)\n",
    "    elif config[\"algorithm\"] == \"SAC\":\n",
    "        model = SAC(config[\"policy\"], env, verbose=1, batch_size=config[\"batch_size\"], tensorboard_log=tensor_logger, device=device)\n",
    "    print(\"Created a new model\")\n",
    "else:\n",
    "    print(\"Loading existing model\")\n",
    "    '''\n",
    "    ## Load the latest model\n",
    "    model_latest_checkpoint = find_latest_checkpoint(log_dir)\n",
    "    print(\"Loading model from\", model_latest_checkpoint)\n",
    "    env_last_checkpoint = find_latest_checkpoint(log_dir, object_type=\"_vecnormalize\", file_type=\".pkl\")\n",
    "    replay_latest_checkpoint = find_latest_checkpoint(log_dir, object_type=\"_replay_buffer\", file_type=\".pkl\")\n",
    "    '''\n",
    "    model_latest_checkpoint = os.path.join(\".\", config[\"model_file_name\"] + '.zip')\n",
    "    env_last_checkpoint = os.path.join(\".\", 'vec_normalize_' + config[\"model_file_name\"] + '.pkl')\n",
    "\n",
    "    if config[\"normalize\"]:\n",
    "        #env = VecNormalize.load(env_last_checkpoint, env)\n",
    "        env.load(env_last_checkpoint, env)\n",
    "                \n",
    "    if config[\"algorithm\"] == \"PPO\": #Create to-be-trained model with the the PPO algorithm and an actor-critic policy using multi-layer perceptrons\n",
    "        model = PPO.load(model_latest_checkpoint, env=env, tensorboard_log=tensor_logger, device=device)\n",
    "    elif config[\"algorithm\"] == \"DDPG\": #Create to-be-trained model with the DDPG algorithm using multi-layer perceptrons\n",
    "        model = DDPG.load(model_latest_checkpoint, env=env,tensorboard_log=tensor_logger, device=device)\n",
    "    elif config[\"algorithm\"] == \"SAC\": #Create to-be-trained model with the SAC algorithm using multi-layer perceptrons\n",
    "        model = SAC.load(model_latest_checkpoint, env=env, tensorboard_log=tensor_logger, device=device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid algorithm specified in the configuration.\")\n",
    "    '''\n",
    "    if replay_latest_checkpoint is not None:\n",
    "        model.load_replay_buffer(replay_latest_checkpoint)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training loop trains model for 1000000 steps every iteration and runs for 10 iterations\n",
    "During each step, an action is predicted by the model, and the environment state is updated\n",
    "'''\n",
    "if config[\"training\"]:\n",
    "    '''\n",
    "    class CustomCheckpointCallback(CheckpointCallback):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super(CustomCheckpointCallback, self).__init__(*args, **kwargs)\n",
    "\n",
    "        def _on_step(self) -> bool:\n",
    "            try:\n",
    "                return super(CustomCheckpointCallback, self)._on_step()\n",
    "            except TypeError as e:\n",
    "                print(f\"Serialization error encountered: {e}\")\n",
    "                return True  # Continue training despite the error\n",
    "    \n",
    "    # Save a checkpoint every 1000 steps\n",
    "    checkpoint_callback = CustomCheckpointCallback(\n",
    "        save_freq=2048,\n",
    "        save_path=\"./logs/\",\n",
    "        name_prefix=f\"{config['algorithm']}_model\",\n",
    "        save_replay_buffer=True,\n",
    "        save_vecnormalize=True,\n",
    "        verbose=0,\n",
    "        )  \n",
    "    '''\n",
    "    # Save during training the best model based on the evaluation score\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
    "                             log_path=\"./logs/\", eval_freq=2048,\n",
    "                             deterministic=True, render=False,\n",
    "                             verbose=0)\n",
    "    \n",
    "    # Create the callback list\n",
    "    #callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "    callback = eval_callback\n",
    "\n",
    "    # Define the model with the TensorBoard logger\n",
    "    #model.set_logger(tensor_logger)\n",
    "    \n",
    "    # Define the number of training repetitions\n",
    "    training_repetitions = config[\"training_repetitions\"]\n",
    "    total_timesteps = config[\"training_total_timesteps\"]\n",
    "\n",
    "    if config[\"algorithm\"] == \"PPO\":\n",
    "        # Define steps per iteration\n",
    "        n_steps_per_iteration = model.n_steps\n",
    "        n_steps_per_iteration_all_envs = n_steps_per_iteration * config[\"num_envs\"]\n",
    "\n",
    "        # Calculate number of iterations\n",
    "        num_iterations = total_timesteps // n_steps_per_iteration\n",
    "        print(f\"Total timesteps: {total_timesteps}, Steps per iteration: {n_steps_per_iteration}, Number of iterations: {num_iterations}\")\n",
    "\n",
    "        for i in range(num_iterations):  # Number of learning iterations\n",
    "            print(f\"Starting learning iteration {i}\")\n",
    "            #model.learn(total_timesteps=n_steps_per_iteration_all_envs, callback=callback, progress_bar=True)\n",
    "            model.learn(total_timesteps=n_steps_per_iteration_all_envs, tb_log_name=\"H_3000_CF_100\", callback=callback)\n",
    "            model.save(config[\"model_file_name\"] + '.zip')\n",
    "            if config[\"normalize\"]:\n",
    "                env.save('vec_normalize_' + config[\"model_file_name\"] +'.pkl')\n",
    "            print(f\"Completed learning iteration {i} successfully\")\n",
    "\n",
    "    else:\n",
    "        for i in range(training_repetitions):  # Number of learning iterations\n",
    "            print(f\"Starting learning iteration {i}\")\n",
    "            model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n",
    "            model.save(config[\"model_file_name\"] + '.zip')\n",
    "            if config[\"normalize\"]:\n",
    "                env.save('vec_normalize_' + config[\"model_file_name\"] +'.pkl')\n",
    "            print(f\"Completed learning iteration {i} successfully\")\n",
    "             \n",
    "#Load the best model based on the evaluation score (total_reward):\n",
    "model = PPO.load('./logs/best_model' + '.zip', env=env, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by simulating and visualize actions in renderers\n",
    "episode_rewards = []\n",
    "evaluation_repetitions = config[\"evaluation_repetitions\"]\n",
    "for i_episode in range(evaluation_repetitions):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(2048):\n",
    "        env.render()\n",
    "        #print(obs)\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done.all():\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "average_reward_per_environment = sum(episode_rewards) / len(episode_rewards)\n",
    "average_reward = np.mean(average_reward_per_environment)\n",
    "print(f\"Iteration {i_episode+1}/{range(3)}, Average Reward per Environment: {average_reward_per_environment}, Average Reward: {average_reward}\")\n",
    "\n",
    "#Close environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IKfIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
