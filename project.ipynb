{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Pick-and-Place Environment in Robosuite\n",
    "\n",
    "## Abstract\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Blah Blah about robosuite and Stable Baselines\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "1. Import numpy and robosuite\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load configuration file config.yaml\n",
      "Running environment with training = True  and simulation = True\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import robosuite as suite\n",
    "\n",
    "from gym.spaces import Box\n",
    "\n",
    "from robosuite import load_controller_config\n",
    "from robosuite.environments.base import register_env\n",
    "from robosuite.controllers import load_controller_config, ALL_CONTROLLERS\n",
    "from robosuite.wrappers import GymWrapper\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from rlkit.launchers.launcher_util import setup_logger\n",
    "from rlkit.envs.wrappers import NormalizedBoxEnv\n",
    "from rlkit.torch.sac.sac import SACTrainer\n",
    "from rlkit.torch.networks import FlattenMlp, TanhMlpPolicy\n",
    "from rlkit.data_management.env_replay_buffer import EnvReplayBuffer\n",
    "from rlkit.exploration_strategies.base import PolicyWrappedWithExplorationStrategy\n",
    "from rlkit.samplers.data_collector import MdpPathCollector\n",
    "from rlkit.torch.sac.policies import TanhGaussianPolicy, MakeDeterministic\n",
    "from rlkit.exploration_strategies.gaussian_strategy import GaussianStrategy\n",
    "from rlkit_custom import CustomTorchBatchRLAlgorithm\n",
    "\n",
    "config = {}\n",
    "print('Load configuration file config.yaml')\n",
    "with open(\"config.yaml\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as e:\n",
    "        print(e)\n",
    "\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "THIS_DIR = os.getcwd()\n",
    "\n",
    "print(\"Running environment with training =\", str(config[\"training\"]), \" and simulation =\", str(config[\"simulation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-18 14:46:52.952848 CEST | [PickPlace_IIWA_OSC_POSE_SEED554641_2024_07_18_14_25_16_0000--s-0] [PickPlace_IIWA_OSC_POSE_SEED554641_2024_07_18_14_27_14_0000--s-0] Variant:\n",
      "2024-07-18 14:46:52.953593 CEST | [PickPlace_IIWA_OSC_POSE_SEED554641_2024_07_18_14_25_16_0000--s-0] [PickPlace_IIWA_OSC_POSE_SEED554641_2024_07_18_14_27_14_0000--s-0] {\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"seed\": 554641,\n",
      "  \"version\": \"normal\",\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ]\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ]\n",
      "  },\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"num_epochs\": 2000,\n",
      "    \"num_eval_steps_per_epoch\": 5000,\n",
      "    \"num_trains_per_train_loop\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 5000,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"expl_max_path_length\": 500,\n",
      "    \"eval_max_path_length\": 500,\n",
      "    \"batch_size\": 256\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.0003,\n",
      "    \"qf_lr\": 0.0003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": false\n",
      "  },\n",
      "  \"expl_environment_kwargs\": {\n",
      "    \"env_name\": \"PickPlace\",\n",
      "    \"robots\": \"IIWA\",\n",
      "    \"controller\": \"OSC_POSE\",\n",
      "    \"horizon\": 500,\n",
      "    \"control_freq\": 20,\n",
      "    \"reward_scale\": 1,\n",
      "    \"hard_reset\": true,\n",
      "    \"ignore_done\": true\n",
      "  },\n",
      "  \"eval_environment_kwargs\": {\n",
      "    \"env_name\": \"PickPlace\",\n",
      "    \"robots\": \"IIWA\",\n",
      "    \"controller\": \"OSC_POSE\",\n",
      "    \"horizon\": 500,\n",
      "    \"control_freq\": 20,\n",
      "    \"reward_scale\": 1,\n",
      "    \"hard_reset\": true,\n",
      "    \"ignore_done\": true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# create environment instance\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    discount=0.99,\n",
    "    soft_target_tau=5e-3,\n",
    "    target_update_period=1,\n",
    "    policy_lr=3e-4,\n",
    "    qf_lr=3e-4,\n",
    "    reward_scale=1,\n",
    "    use_automatic_entropy_tuning=(not True),\n",
    ")\n",
    "\n",
    "expl_env_kwargs = dict(\n",
    "    env_name=\"PickPlace\",\n",
    "    robots=config[\"robot_name\"],\n",
    "    controller=config[\"robot_controller\"],\n",
    "    horizon=500,\n",
    "    control_freq=20,\n",
    "    reward_scale=1,\n",
    "    hard_reset=True,\n",
    "    ignore_done=True,\n",
    ")\n",
    "\n",
    "eval_env_kwargs = dict(\n",
    "    env_name=\"PickPlace\",\n",
    "    robots=config[\"robot_name\"],\n",
    "    controller=config[\"robot_controller\"],\n",
    "    horizon=500,\n",
    "    control_freq=20,\n",
    "    reward_scale=1,\n",
    "    hard_reset=True,\n",
    "    ignore_done=True,\n",
    ")\n",
    "\n",
    "variant = dict(\n",
    "    algorithm=\"SAC\",\n",
    "    seed=config[\"seed\"],\n",
    "    version=\"normal\",\n",
    "    replay_buffer_size=int(1E6),\n",
    "    qf_kwargs=dict(\n",
    "        hidden_sizes=[256,256],\n",
    "    ),\n",
    "    policy_kwargs=dict(\n",
    "        hidden_sizes=[256,256],\n",
    "    ),\n",
    "    algorithm_kwargs=dict(\n",
    "        num_epochs=2000, # args.n_epochs\n",
    "        num_eval_steps_per_epoch=500 * 10, #args.eval_horizon * args.num_eval\n",
    "        num_trains_per_train_loop=1000,\n",
    "        num_expl_steps_per_train_loop= 500 * 10,   # args.expl_horizon * args.expl_ep_per_train_loop\n",
    "        min_num_steps_before_training=1000,\n",
    "        expl_max_path_length=500, #args.expl_horizon\n",
    "        eval_max_path_length=500, #args.eval_horizon\n",
    "        batch_size=256,\n",
    "    ),\n",
    "    trainer_kwargs=trainer_kwargs,\n",
    "    expl_environment_kwargs=expl_env_kwargs,\n",
    "    eval_environment_kwargs=eval_env_kwargs,\n",
    ")\n",
    "\n",
    "ptu.set_gpu_mode(torch.cuda.is_available())\n",
    "\n",
    "tmp_file_prefix = \"{}_{}_{}_SEED{}\".format(\"PickPlace\", \"\".join(config[\"robot_name\"]), config[\"robot_controller\"], config[\"seed\"])\n",
    "\n",
    "abs_root_dir = os.path.join(THIS_DIR, \"logs\")\n",
    "tmp_dir = setup_logger(tmp_file_prefix, variant=variant, base_log_dir=abs_root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobi/Documents/IKfIR/lib/python3.11/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unknown space: Box(-inf, inf, (96,), float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m\n\u001b[1;32m     53\u001b[0m eval_policy \u001b[38;5;241m=\u001b[39m MakeDeterministic(expl_policy)\n\u001b[1;32m     54\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SACTrainer(\n\u001b[1;32m     55\u001b[0m     env\u001b[38;5;241m=\u001b[39meval_env,\n\u001b[1;32m     56\u001b[0m     policy\u001b[38;5;241m=\u001b[39mexpl_policy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvariant[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainer_kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 64\u001b[0m replay_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mEnvReplayBuffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1E6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpl_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m eval_path_collector \u001b[38;5;241m=\u001b[39m MdpPathCollector(\n\u001b[1;32m     69\u001b[0m     eval_env,\n\u001b[1;32m     70\u001b[0m     eval_policy,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m expl_path_collector \u001b[38;5;241m=\u001b[39m MdpPathCollector(\n\u001b[1;32m     73\u001b[0m     expl_env,\n\u001b[1;32m     74\u001b[0m     expl_policy,\n\u001b[1;32m     75\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/toolkit/rlkit/rlkit/data_management/env_replay_buffer.py:31\u001b[0m, in \u001b[0;36mEnvReplayBuffer.__init__\u001b[0;34m(self, max_replay_buffer_size, env, env_info_sizes)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         env_info_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     30\u001b[0m     max_replay_buffer_size\u001b[38;5;241m=\u001b[39mmax_replay_buffer_size,\n\u001b[0;32m---> 31\u001b[0m     observation_dim\u001b[38;5;241m=\u001b[39m\u001b[43mget_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ob_space\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     32\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39mget_dim(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space),\n\u001b[1;32m     33\u001b[0m     env_info_sizes\u001b[38;5;241m=\u001b[39menv_info_sizes\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/toolkit/rlkit/rlkit/envs/env_utils.py:22\u001b[0m, in \u001b[0;36mget_dim\u001b[0;34m(space)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m space\u001b[38;5;241m.\u001b[39mflat_dim\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown space: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(space))\n",
      "\u001b[0;31mTypeError\u001b[0m: Unknown space: Box(-inf, inf, (96,), float32)"
     ]
    }
   ],
   "source": [
    "if config[\"training\"]:\n",
    "    \n",
    "    # Get environment configs for expl and eval envs and create the appropriate envs\n",
    "    # suites[0] is expl and suites[1] is eval\n",
    "    suites = []\n",
    "    for env_config in (variant[\"expl_environment_kwargs\"], variant[\"eval_environment_kwargs\"]):\n",
    "        # Load controller\n",
    "        controller = env_config.pop(\"controller\")\n",
    "        controller_config = load_controller_config(default_controller=controller)\n",
    "        # Create robosuite env and append to our list\n",
    "        suites.append(suite.make(**env_config,\n",
    "                                 has_renderer=False,\n",
    "                                 has_offscreen_renderer=False,\n",
    "                                 use_object_obs=True,\n",
    "                                 use_camera_obs=False,\n",
    "                                 reward_shaping=True,\n",
    "                                 controller_configs=controller_config,\n",
    "                                 ))\n",
    "    # Create gym-compatible envs\n",
    "    expl_env = NormalizedBoxEnv(GymWrapper(suites[0]))\n",
    "    eval_env = NormalizedBoxEnv(GymWrapper(suites[1]))\n",
    "\n",
    "    obs_dim = expl_env.observation_space.low.size\n",
    "    action_dim = eval_env.action_space.low.size\n",
    "\n",
    "    qf1 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        **variant['qf_kwargs'],\n",
    "    )\n",
    "    qf2 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        **variant['qf_kwargs'],\n",
    "    )\n",
    "    target_qf1 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        **variant['qf_kwargs'],\n",
    "    )\n",
    "    target_qf2 = FlattenMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        **variant['qf_kwargs'],\n",
    "    )\n",
    "\n",
    "    # Instantiate trainer with appropriate agent\n",
    "    expl_policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        **variant['policy_kwargs'],\n",
    "    )\n",
    "    eval_policy = MakeDeterministic(expl_policy)\n",
    "    trainer = SACTrainer(\n",
    "        env=eval_env,\n",
    "        policy=expl_policy,\n",
    "        qf1=qf1,\n",
    "        qf2=qf2,\n",
    "        target_qf1=target_qf1,\n",
    "        target_qf2=target_qf2,\n",
    "        **variant['trainer_kwargs']\n",
    "    )\n",
    "\n",
    "    # Adjust observation space if needed\n",
    "    obs_space = expl_env.observation_space\n",
    "    if isinstance(obs_space, Box):\n",
    "        low = np.where(obs_space.low == -np.inf, -1e10, obs_space.low)\n",
    "        high = np.where(obs_space.high == np.inf, 1e10, obs_space.high)\n",
    "        expl_env.observation_space = Box(low, high, dtype=obs_space.dtype)\n",
    "\n",
    "    # Adjust action space if needed\n",
    "    action_space = expl_env.action_space\n",
    "    if isinstance(action_space, Box):\n",
    "        low = np.where(action_space.low == -np.inf, -1e10, action_space.low)\n",
    "        high = np.where(action_space.high == np.inf, 1e10, action_space.high)\n",
    "        expl_env.action_space = Box(low, high, dtype=action_space.dtype)\n",
    "\n",
    "\n",
    "    replay_buffer = EnvReplayBuffer(\n",
    "        variant['replay_buffer_size'],\n",
    "        expl_env,\n",
    "    )\n",
    "    eval_path_collector = MdpPathCollector(\n",
    "        eval_env,\n",
    "        eval_policy,\n",
    "    )\n",
    "    expl_path_collector = MdpPathCollector(\n",
    "        expl_env,\n",
    "        expl_policy,\n",
    "    )\n",
    "\n",
    "    # Define algorithm\n",
    "    algorithm = CustomTorchBatchRLAlgorithm(\n",
    "        trainer=trainer,\n",
    "        exploration_env=expl_env,\n",
    "        evaluation_env=eval_env,\n",
    "        exploration_data_collector=expl_path_collector,\n",
    "        evaluation_data_collector=eval_path_collector,\n",
    "        replay_buffer=replay_buffer,\n",
    "        **variant['algorithm_kwargs']\n",
    "    )\n",
    "    algorithm.to(ptu.device)\n",
    "    algorithm.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting learning iteration 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IKfIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
