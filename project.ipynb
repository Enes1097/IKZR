{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Pick-and-Place Environment in Robosuite\n",
    "\n",
    "## Abstract\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Blah Blah about robosuite and Stable Baselines\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "1. Import numpy and robosuite\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import robosuite as suite\n",
    "\n",
    "\n",
    "from robosuite import load_controller_config\n",
    "from robosuite.environments.base import register_env\n",
    "from robosuite.controllers import load_controller_config\n",
    "from stable_baselines3.common.save_util import save_to_zip_file, load_from_zip_file\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from robosuite.wrappers import GymWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating Standardized Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment instance\n",
    "controller_config = load_controller_config(default_controller=\"OSC_POSE\")\n",
    "\n",
    "\n",
    "def reward(self, action=None):\n",
    "    reward = 1\n",
    "    return reward\n",
    "\n",
    "def make_robosuite_env(env_id, options, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param options: (dict) additional arguments to pass to the specific environment class initializer\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = GymWrapper(suite.make(env_id, **options))\n",
    "        env.render_mode = 'mujoco'\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Simulate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_options = {\"robots\":\"IIWA\",\n",
    "    \"controller_configs\":controller_config,\n",
    "    \"has_renderer\":True,\n",
    "    \"has_offscreen_renderer\":False,\n",
    "    \"use_camera_obs\":False,\n",
    "    \"reward_shaping\":True,\n",
    "    }\n",
    "\n",
    "continue_training_model_filename = \"model\"\n",
    "#continue_training_model_filename = None\n",
    "\n",
    "\n",
    "env = SubprocVecEnv([make_robosuite_env(\"PickPlace\", env_options, i, 0) for i in range(3)])\n",
    "\n",
    "# Create callback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000000, save_path='./checkpoints/', \n",
    "                        name_prefix=\"model\", verbose=2)\n",
    "    \n",
    "        # Train new model\n",
    "if continue_training_model_filename is None:\n",
    "\n",
    "    # Normalize environment\n",
    "    env = VecNormalize(env)\n",
    "    #env.render_mode = \"mujoco\"\n",
    "\n",
    "    # Create model\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "    print(\"Created a new model\")\n",
    "\n",
    "# Continual training\n",
    "else:\n",
    "\n",
    "    # Join file paths\n",
    "    continue_training_model_path = os.path.join(\".\", continue_training_model_filename + '.zip')\n",
    "    continue_training_vecnormalize_path = os.path.join(\".\", 'vec_normalize_' + continue_training_model_filename + '.pkl')\n",
    "\n",
    "    print(f\"Continual training on model located at {continue_training_model_path}\")\n",
    "\n",
    "    # Load normalized env \n",
    "    env = VecNormalize.load(continue_training_vecnormalize_path, env)\n",
    "\n",
    "    # Load model\n",
    "    model = PPO.load(continue_training_model_path, env=env)\n",
    "\n",
    "\n",
    "for i in range(0):\n",
    "    print(\"Starting learning iteration \" + str(i))\n",
    "    model.learn(total_timesteps=100000)\n",
    "    model.save('model.zip')\n",
    "    print(\"Saved learing iteration \" + str(i) + \" successfully\")\n",
    "    env.save('vec_normalize_model.pkl')\n",
    "\n",
    "vec_env = model.get_env()\n",
    "vec_env.render_mode = 'mujoco'\n",
    "obs = vec_env.reset()\n",
    "for i in range(10000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render_mode = 'mujoco'\n",
    "    vec_env.render()\n",
    "\n",
    "vec_env.close\n",
    "env.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IKfIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
